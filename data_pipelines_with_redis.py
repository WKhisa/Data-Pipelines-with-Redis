# -*- coding: utf-8 -*-
"""Data Pipelines with Redis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B_mb0Rbzp0yir55o8d40JcLra9zCryuu
"""

#install
!pip install redis
!pip install psycopg2-binary

#Import Libraries
import pandas as pd
import redis
import psycopg2
import io

# Set Redis Cloud 
redis_host = 'redis-13623.c246.us-east-1-4.ec2.cloud.redislabs.com'
redis_port = 13623
redis_password = 'NLomzJHfXLVnntKlQmjDAQ8Dqm7JHJTw'

# Set PostgreSQL Docker container configuration
pg_host = 'localhost'
pg_port = 5432
pg_user = 'postgres'
pg_password = 'password'
pg_dbname = 'postgres'

#Function to extract and cache data
def extract_data():
    # Extract data from CSV file using pandas
    df = pd.read_csv('customer_call_logs.csv')
    cache_string = df.to_csv(index=False)
    # Cache data in Redis for faster retrieval
    r = redis.Redis(host=redis_host, port=redis_port, password=redis_password)
    r.set('mytable', cache_string)

#Retrieve and Transform Cached Data
def transform_data():
    # Retrieve data from Redis cache
    retrieved_string = r.get('mytable').decode('utf-8')
    transformed_data = pd.read_csv(io.StringIO(retrieved_string))

    # Transform data (clean, structure, format)
    transformed_data['call_cost'] = transformed_data['call_cost'].str.replace('$','' ).astype(float64)
    transformed_data['call_date'] = pd.to_datetime(transformed_data['call_date'])
    transformed_data['call_duration'] = pd.to_datetime(transformed_data['call_duration'])

    return transformed_data

#Function to load to Postgress
def load_data():
    # Connect to Postgres database using cursor object
    conn = psycopg2.connect(user=pg_user, password=pg_password, host=pg_host, port=pg_port, database=pg_dbname)
    cur = conn.cursor()    

    # Create a table to store the data
    cur.execute('CREATE TABLE IF NOT EXISTS customer_call_logs (customer_id INT,call_cost_usd FLOAT,call_destination VARCHAR,call_date TIMESTAMP, call_duration_min FLOAT)')


    # Insert the transformed data into the database
    for i, row in transformed_data.iterrows():
        cur.execute(f"INSERT INTO customer_call_logs (customer_id, call_cost_usd, call_destination, call_date, call_duration_min) VALUES ({row['customer_id']}, {row['call_cost_usd']}, '{row['call_destination']}', '{row['call_date']}', {row['call_duration_min']})")

    # Commit the changes
    conn.commit()

    # Close the cursor and connection
    cur.close()
    conn.close()

def data_pipeline():
    # Data pipeline function
    extract_data()
    transformed_data = transform_data()
    load_data(transformed_data)

if __name__ == '__main__':
    # Run the data pipeline function
    data_pipeline()